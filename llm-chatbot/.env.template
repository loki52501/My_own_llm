# Model Configuration
MODEL_NAME=mistralai/Mistral-7B-v0.1
MODEL_MAX_LENGTH=2048

# Training Configuration
LEARNING_RATE=2e-4
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
NUM_EPOCHS=3
WARMUP_STEPS=100

# LoRA Configuration
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05

# Quantization
LOAD_IN_4BIT=true
LOAD_IN_8BIT=false

# Logging
WANDB_PROJECT=llm-chatbot
WANDB_API_KEY=your_wandb_api_key_here
LOG_STEPS=10
SAVE_STEPS=500

# Inference
MAX_NEW_TOKENS=512
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_SECRET_KEY=your-secret-key-change-this

# Paperspace
HF_HOME=./cache
TRANSFORMERS_CACHE=./cache