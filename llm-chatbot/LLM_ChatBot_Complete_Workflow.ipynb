{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM ChatBot - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete pipeline for building a ChatGPT-like LLM system on Paperspace.\n",
    "\n",
    "## Steps:\n",
    "1. Setup and verification\n",
    "2. Dataset preparation\n",
    "3. Model fine-tuning with LoRA\n",
    "4. Inference and testing\n",
    "5. Deployment (Gradio & API)\n",
    "\n",
    "**Estimated Time**: 2-4 hours (depending on dataset size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T18:22:10.039035Z",
     "iopub.status.busy": "2025-12-26T18:22:10.038412Z",
     "iopub.status.idle": "2025-12-26T18:22:11.425777Z",
     "shell.execute_reply": "2025-12-26T18:22:11.425134Z",
     "shell.execute_reply.started": "2025-12-26T18:22:10.039009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0]\n",
      "PyTorch version: 2.1.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU count: 1\n",
      "GPU 0: NVIDIA RTX A4000\n",
      "  Memory: 15.72 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"  Memory: {gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T18:22:26.016266Z",
     "iopub.status.busy": "2025-12-26T18:22:26.015546Z",
     "iopub.status.idle": "2025-12-26T18:22:26.142496Z",
     "shell.execute_reply": "2025-12-26T18:22:26.141774Z",
     "shell.execute_reply.started": "2025-12-26T18:22:26.016240Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T18:22:27.822361Z",
     "iopub.status.busy": "2025-12-26T18:22:27.821848Z",
     "iopub.status.idle": "2025-12-26T18:22:27.825982Z",
     "shell.execute_reply": "2025-12-26T18:22:27.825420Z",
     "shell.execute_reply.started": "2025-12-26T18:22:27.822339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"./cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache\"\n",
    "\n",
    "# Optional: Set your HuggingFace token if using gated models (e.g., Llama 2)\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "We'll use the Alpaca dataset for instruction fine-tuning. Start with a small subset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T18:22:30.396597Z",
     "iopub.status.busy": "2025-12-26T18:22:30.395827Z",
     "iopub.status.idle": "2025-12-26T18:22:30.552875Z",
     "shell.execute_reply": "2025-12-26T18:22:30.552294Z",
     "shell.execute_reply.started": "2025-12-26T18:22:30.396570Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prepare_dataset_notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprepare_dataset_notebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Prepare dataset (start with 1000 samples for testing)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m prepare_dataset(\n\u001b[1;32m      5\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtatsu-lab/alpaca\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     template_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpaca\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     preview\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prepare_dataset_notebook'"
     ]
    }
   ],
   "source": [
    "from prepare_dataset_notebook import prepare_dataset\n",
    "\n",
    "# Prepare dataset (start with 1000 samples for testing)\n",
    "dataset = prepare_dataset(\n",
    "    dataset_name=\"tatsu-lab/alpaca\",\n",
    "    template_name=\"alpaca\",\n",
    "    max_samples=1000,  # Use 1000 for quick test, None for full dataset\n",
    "    output_dir=\"./data/processed\",\n",
    "    preview=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset prepared!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample\n",
    "print(\"Sample training example:\")\n",
    "print(\"=\" * 80)\n",
    "print(dataset['train'][0]['text'][:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Fine-Tuning with LoRA\n",
    "\n",
    "Train the model using QLoRA for memory efficiency. This will take the most time (~15 mins for 1000 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_lora import LLMTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = LLMTrainer(\n",
    "    model_name=\"mistralai/Mistral-7B-v0.1\",  # Or \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" for faster testing\n",
    "    dataset_path=\"./data/processed\",\n",
    "    output_dir=\"./models/checkpoints\",\n",
    "    final_model_dir=\"./models/final\",\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=3,  # Reduce to 1 for quick test\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_length=2048,\n",
    "    use_wandb=False,  # Set to True if you have W&B API key\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "trainer.load_model_and_tokenizer()\n",
    "print(\"\\n‚úÖ Model and tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "trainer.configure_lora()\n",
    "print(\"\\n‚úÖ LoRA configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trainer.load_dataset()\n",
    "print(\"\\n‚úÖ Dataset loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (this will take time!)\n",
    "print(\"Starting training...\")\n",
    "print(\"This may take 15-30 minutes for 1000 samples, 4-8 hours for full dataset.\")\n",
    "print(\"You can monitor progress in the output below.\\n\")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ Training complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Testing\n",
    "\n",
    "Now let's test the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import create_chatbot\n",
    "\n",
    "# Create chatbot with fine-tuned model\n",
    "bot = create_chatbot(\n",
    "    model_name=\"mistralai/Mistral-7B-v0.1\",\n",
    "    adapter_path=\"./models/final\",  # LoRA adapter\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChatBot loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single-turn generation\n",
    "response = bot.chat(\n",
    "    \"What is machine learning?\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"User: What is machine learning?\")\n",
    "print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-turn conversation\n",
    "bot.reset_conversation()\n",
    "\n",
    "questions = [\n",
    "    \"Hello! Can you help me understand neural networks?\",\n",
    "    \"What are the main components?\",\n",
    "    \"Can you give a simple example?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = bot.chat(question, max_new_tokens=200)\n",
    "    print(f\"\\nUser: {question}\")\n",
    "    print(f\"Assistant: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View conversation history\n",
    "history = bot.get_conversation_history()\n",
    "print(f\"\\nConversation length: {len(history)} messages\")\n",
    "\n",
    "for i, msg in enumerate(history):\n",
    "    print(f\"{i+1}. {msg['role']}: {msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Testing\n",
    "\n",
    "Try different generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different temperatures\n",
    "prompt = \"Write a creative story about AI\"\n",
    "\n",
    "print(\"Low temperature (more focused):\")\n",
    "print(\"=\" * 80)\n",
    "response_low = bot.generate(prompt, temperature=0.3, max_new_tokens=150)\n",
    "print(response_low)\n",
    "\n",
    "print(\"\\n\\nHigh temperature (more creative):\")\n",
    "print(\"=\" * 80)\n",
    "response_high = bot.generate(prompt, temperature=1.2, max_new_tokens=150)\n",
    "print(response_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Safety and Content Filtering\n",
    "\n",
    "Add safety features to your chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safety_utils import ContentFilter, UsageLogger\n",
    "\n",
    "# Initialize safety features\n",
    "content_filter = ContentFilter(\n",
    "    max_input_length=2048,\n",
    "    blocked_words=[],  # Add words to block\n",
    ")\n",
    "\n",
    "usage_logger = UsageLogger()\n",
    "\n",
    "print(\"‚úÖ Safety features initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe chat function\n",
    "def safe_chat(user_input, bot, filter, logger):\n",
    "    \"\"\"Chat with safety checks\"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Validate input\n",
    "    is_valid, error = filter.validate_input(user_input)\n",
    "    if not is_valid:\n",
    "        print(f\"‚ùå Error: {error}\")\n",
    "        return None\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    response = bot.chat(user_input)\n",
    "    duration = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Log interaction\n",
    "    filter.log_interaction(user_input, response)\n",
    "    logger.log_request(\n",
    "        endpoint=\"chat\",\n",
    "        tokens_used=len(response.split()),\n",
    "        duration_ms=duration\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test safe chat\n",
    "response = safe_chat(\n",
    "    \"Tell me about quantum computing\",\n",
    "    bot,\n",
    "    content_filter,\n",
    "    usage_logger\n",
    ")\n",
    "\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch Web Interface\n",
    "\n",
    "Deploy your chatbot with Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app_gradio import launch_gradio\n",
    "\n",
    "# Launch Gradio interface\n",
    "# This will create a public link if share=True\n",
    "launch_gradio(\n",
    "    model_name=\"mistralai/Mistral-7B-v0.1\",\n",
    "    adapter_path=\"./models/final\",\n",
    "    load_in_4bit=True,\n",
    "    share=True,  # Creates public link (optional)\n",
    "    port=7860\n",
    ")\n",
    "\n",
    "# Note: This will block the notebook. Stop the cell to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch API Server (Alternative)\n",
    "\n",
    "Run FastAPI server for REST API access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this in the same notebook as Gradio\n",
    "# Use this in a separate notebook or terminal\n",
    "\n",
    "# from app_api import launch_api\n",
    "# \n",
    "# launch_api(\n",
    "#     host=\"0.0.0.0\",\n",
    "#     port=8000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test API (if running)\n",
    "\n",
    "Test the API endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Set your API key\n",
    "API_KEY = os.getenv(\"API_SECRET_KEY\", \"your-secret-key-change-this\")\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(f\"{API_URL}/health\")\n",
    "print(\"Health check:\")\n",
    "print(response.json())\n",
    "\n",
    "# Test chat endpoint\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "data = {\n",
    "    \"message\": \"What is deep learning?\",\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_URL}/chat\",\n",
    "    headers=headers,\n",
    "    json=data\n",
    ")\n",
    "\n",
    "print(\"\\nChat response:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "‚úÖ You've completed:\n",
    "1. Environment setup and verification\n",
    "2. Dataset preparation\n",
    "3. Model fine-tuning with LoRA/QLoRA\n",
    "4. Inference and testing\n",
    "5. Safety features implementation\n",
    "6. Web interface deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Train on more data**: Increase `max_samples` to use full dataset\n",
    "2. **Experiment with models**: Try different base models (Llama 2, Phi-2)\n",
    "3. **Tune hyperparameters**: Adjust LoRA rank, learning rate, epochs\n",
    "4. **Add custom dataset**: Prepare your own instruction dataset\n",
    "5. **Deploy to production**: Use Docker for deployment\n",
    "6. **Monitor performance**: Set up W&B or TensorBoard\n",
    "7. **Export models**: Convert to GGUF for llama.cpp\n",
    "\n",
    "### Resources:\n",
    "- README: `README_LLM_CHATBOT.md`\n",
    "- Configuration: `config_training.yaml`, `config_inference.yaml`\n",
    "- Logs: `./logs/training/`, `./logs/inference/`\n",
    "- Models: `./models/final/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
